{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2861e23",
   "metadata": {},
   "source": [
    "### Read data, Preprocess and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17207531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\AppData\\Local\\Temp\\ipykernel_18172\\723767195.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = df.groupby(\"Sentence #\").apply(agg_func).tolist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Tags: ['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/ner_dataset.csv\", encoding=\"latin1\").ffill()\n",
    "\n",
    "# Group by sentence\n",
    "agg_func = lambda s: {\"tokens\": s[\"Word\"].tolist(), \"ner_tags\": s[\"Tag\"].tolist()}\n",
    "grouped = df.groupby(\"Sentence #\").apply(agg_func).tolist()\n",
    "\n",
    "dataset = Dataset.from_list(grouped)\n",
    "\n",
    "# Train/val/test split\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "temp = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": temp[\"train\"],\n",
    "    \"validation\": temp[\"test\"],\n",
    "    \"test\": dataset[\"test\"]\n",
    "})\n",
    "\n",
    "# Get unique tags\n",
    "unique_tags = sorted({tag for doc in dataset[\"train\"][\"ner_tags\"] for tag in doc})\n",
    "tag2id = {t: i for i, t in enumerate(unique_tags)}\n",
    "id2tag = {i: t for t, i in tag2id.items()}\n",
    "\n",
    "print(\"NER Tags:\", unique_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41d64b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f41cd533e6944c682fe36379f2ebb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db54819353a6493aad32ca9b6c2db945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3837 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57495ca5f2984d49a958319f566caf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  # ignore\n",
    "            elif word_id != previous_word:\n",
    "                label_ids.append(tag2id[label[word_id]])\n",
    "            else:\n",
    "                label_ids.append(tag2id[label[word_id]] if label[word_id].startswith(\"I-\") else -100)\n",
    "            previous_word = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a85397dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 34530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3837\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9592\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae7ed8",
   "metadata": {},
   "source": [
    "### Build and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "260f1fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(unique_tags),\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58fb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f7b6798a7c4c7caeba58bb54a4f3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1794, 'learning_rate': 4.884205650764243e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1394, 'learning_rate': 4.7684113015284854e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1274, 'learning_rate': 4.652616952292728e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1176, 'learning_rate': 4.536822603056971e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf23f2e040b42b387ca4f253616b72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5714285714285714, 'recall': 0.13793103448275862, 'f1': 0.2222222222222222, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8467289719626169, 'recall': 0.8692037096258395, 'f1': 0.8578191573299668, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9335347432024169, 'recall': 0.9641185647425897, 'f1': 0.9485801995395242, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5, 'recall': 0.4, 'f1': 0.4444444444444445, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6157049375371803, 'recall': 0.6201318154583583, 'f1': 0.617910447761194, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7351313969571232, 'recall': 0.773090909090909, 'f1': 0.7536334633108827, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8857142857142857, 'recall': 0.8269230769230769, 'f1': 0.8553095925569457, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory ./results\\checkpoint-2159 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1136072650551796, 'eval_art': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 33}, 'eval_eve': {'precision': 0.5714285714285714, 'recall': 0.13793103448275862, 'f1': 0.2222222222222222, 'number': 29}, 'eval_geo': {'precision': 0.8467289719626169, 'recall': 0.8692037096258395, 'f1': 0.8578191573299668, 'number': 3127}, 'eval_gpe': {'precision': 0.9335347432024169, 'recall': 0.9641185647425897, 'f1': 0.9485801995395242, 'number': 1282}, 'eval_nat': {'precision': 0.5, 'recall': 0.4, 'f1': 0.4444444444444445, 'number': 15}, 'eval_org': {'precision': 0.6157049375371803, 'recall': 0.6201318154583583, 'f1': 0.617910447761194, 'number': 1669}, 'eval_per': {'precision': 0.7351313969571232, 'recall': 0.773090909090909, 'f1': 0.7536334633108827, 'number': 1375}, 'eval_tim': {'precision': 0.8857142857142857, 'recall': 0.8269230769230769, 'f1': 0.8553095925569457, 'number': 1612}, 'eval_overall_precision': 0.8048541575968655, 'eval_overall_recall': 0.8089039597462262, 'eval_overall_f1': 0.806873977086743, 'eval_overall_accuracy': 0.965038865070236, 'eval_runtime': 26.6802, 'eval_samples_per_second': 143.814, 'eval_steps_per_second': 8.995, 'epoch': 1.0}\n",
      "{'loss': 0.0966, 'learning_rate': 4.421028253821214e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0904, 'learning_rate': 4.3052339045854563e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0882, 'learning_rate': 4.189439555349699e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0891, 'learning_rate': 4.073645206113942e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15ac7460900470799575f7bd57fbcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.4, 'recall': 0.06060606060606061, 'f1': 0.10526315789473685, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.4, 'recall': 0.20689655172413793, 'f1': 0.2727272727272727, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8516009852216748, 'recall': 0.8845538855132715, 'f1': 0.867764705882353, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9631949882537196, 'recall': 0.9594383775351014, 'f1': 0.9613130128956624, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6666666666666666, 'recall': 0.4, 'f1': 0.5, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.664043583535109, 'recall': 0.6572798082684242, 'f1': 0.6606443842216201, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.756114605171209, 'recall': 0.7869090909090909, 'f1': 0.7712045616535994, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8621755253399258, 'recall': 0.8653846153846154, 'f1': 0.8637770897832817, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory ./results\\checkpoint-4318 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10650458931922913, 'eval_art': {'precision': 0.4, 'recall': 0.06060606060606061, 'f1': 0.10526315789473685, 'number': 33}, 'eval_eve': {'precision': 0.4, 'recall': 0.20689655172413793, 'f1': 0.2727272727272727, 'number': 29}, 'eval_geo': {'precision': 0.8516009852216748, 'recall': 0.8845538855132715, 'f1': 0.867764705882353, 'number': 3127}, 'eval_gpe': {'precision': 0.9631949882537196, 'recall': 0.9594383775351014, 'f1': 0.9613130128956624, 'number': 1282}, 'eval_nat': {'precision': 0.6666666666666666, 'recall': 0.4, 'f1': 0.5, 'number': 15}, 'eval_org': {'precision': 0.664043583535109, 'recall': 0.6572798082684242, 'f1': 0.6606443842216201, 'number': 1669}, 'eval_per': {'precision': 0.756114605171209, 'recall': 0.7869090909090909, 'f1': 0.7712045616535994, 'number': 1375}, 'eval_tim': {'precision': 0.8621755253399258, 'recall': 0.8653846153846154, 'f1': 0.8637770897832817, 'number': 1612}, 'eval_overall_precision': 0.8194489465153971, 'eval_overall_recall': 0.8295777729162109, 'eval_overall_f1': 0.8244822525411751, 'eval_overall_accuracy': 0.9670489269987336, 'eval_runtime': 33.1166, 'eval_samples_per_second': 115.863, 'eval_steps_per_second': 7.247, 'epoch': 2.0}\n",
      "{'loss': 0.0784, 'learning_rate': 3.957850856878185e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0603, 'learning_rate': 3.8422880963408985e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0609, 'learning_rate': 3.726493747105141e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0617, 'learning_rate': 3.6106993978693844e-05, 'epoch': 2.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0175a1815e33439291186427699ddbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.5, 'recall': 0.09090909090909091, 'f1': 0.15384615384615385, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.21052631578947367, 'recall': 0.13793103448275862, 'f1': 0.16666666666666666, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8320588235294117, 'recall': 0.9047009913655261, 'f1': 0.8668607323425769, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.977327935222672, 'recall': 0.9414976599063962, 'f1': 0.9590782677791021, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6666666666666666, 'recall': 0.4, 'f1': 0.5, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6406160867084997, 'recall': 0.6728579988016776, 'f1': 0.6563413208649911, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.786096256684492, 'recall': 0.7483636363636363, 'f1': 0.7667660208643815, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8616822429906542, 'recall': 0.857940446650124, 'f1': 0.8598072738576312, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory ./results\\checkpoint-6477 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12094799429178238, 'eval_art': {'precision': 0.5, 'recall': 0.09090909090909091, 'f1': 0.15384615384615385, 'number': 33}, 'eval_eve': {'precision': 0.21052631578947367, 'recall': 0.13793103448275862, 'f1': 0.16666666666666666, 'number': 29}, 'eval_geo': {'precision': 0.8320588235294117, 'recall': 0.9047009913655261, 'f1': 0.8668607323425769, 'number': 3127}, 'eval_gpe': {'precision': 0.977327935222672, 'recall': 0.9414976599063962, 'f1': 0.9590782677791021, 'number': 1282}, 'eval_nat': {'precision': 0.6666666666666666, 'recall': 0.4, 'f1': 0.5, 'number': 15}, 'eval_org': {'precision': 0.6406160867084997, 'recall': 0.6728579988016776, 'f1': 0.6563413208649911, 'number': 1669}, 'eval_per': {'precision': 0.786096256684492, 'recall': 0.7483636363636363, 'f1': 0.7667660208643815, 'number': 1375}, 'eval_tim': {'precision': 0.8616822429906542, 'recall': 0.857940446650124, 'f1': 0.8598072738576312, 'number': 1612}, 'eval_overall_precision': 0.8123393316195373, 'eval_overall_recall': 0.8295777729162109, 'eval_overall_f1': 0.8208680593137785, 'eval_overall_accuracy': 0.9659335169112435, 'eval_runtime': 18.4865, 'eval_samples_per_second': 207.557, 'eval_steps_per_second': 12.982, 'epoch': 3.0}\n",
      "{'loss': 0.0605, 'learning_rate': 3.494905048633627e-05, 'epoch': 3.01}\n",
      "{'loss': 0.0387, 'learning_rate': 3.3791106993978695e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0423, 'learning_rate': 3.263547938860584e-05, 'epoch': 3.47}\n",
      "{'loss': 0.0416, 'learning_rate': 3.147985178323298e-05, 'epoch': 3.71}\n",
      "{'loss': 0.0405, 'learning_rate': 3.0321908290875407e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9376cf970981402e9bd64326ab2f71cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.4166666666666667, 'recall': 0.15151515151515152, 'f1': 0.2222222222222222, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.28, 'recall': 0.2413793103448276, 'f1': 0.25925925925925924, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8627943485086342, 'recall': 0.8787975695554845, 'f1': 0.870722433460076, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9550038789759504, 'recall': 0.9602184087363494, 'f1': 0.957604045118631, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.35294117647058826, 'recall': 0.4, 'f1': 0.37500000000000006, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6637579988365329, 'recall': 0.6836428999400839, 'f1': 0.6735537190082646, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7375838926174496, 'recall': 0.7992727272727272, 'f1': 0.7671902268760908, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8611793611793612, 'recall': 0.869727047146402, 'f1': 0.8654320987654321, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory ./results\\checkpoint-8636 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13224658370018005, 'eval_art': {'precision': 0.4166666666666667, 'recall': 0.15151515151515152, 'f1': 0.2222222222222222, 'number': 33}, 'eval_eve': {'precision': 0.28, 'recall': 0.2413793103448276, 'f1': 0.25925925925925924, 'number': 29}, 'eval_geo': {'precision': 0.8627943485086342, 'recall': 0.8787975695554845, 'f1': 0.870722433460076, 'number': 3127}, 'eval_gpe': {'precision': 0.9550038789759504, 'recall': 0.9602184087363494, 'f1': 0.957604045118631, 'number': 1282}, 'eval_nat': {'precision': 0.35294117647058826, 'recall': 0.4, 'f1': 0.37500000000000006, 'number': 15}, 'eval_org': {'precision': 0.6637579988365329, 'recall': 0.6836428999400839, 'f1': 0.6735537190082646, 'number': 1669}, 'eval_per': {'precision': 0.7375838926174496, 'recall': 0.7992727272727272, 'f1': 0.7671902268760908, 'number': 1375}, 'eval_tim': {'precision': 0.8611793611793612, 'recall': 0.869727047146402, 'f1': 0.8654320987654321, 'number': 1612}, 'eval_overall_precision': 0.8156967431927389, 'eval_overall_recall': 0.8355939619339313, 'eval_overall_f1': 0.8255254768465985, 'eval_overall_accuracy': 0.9660961808823358, 'eval_runtime': 33.9609, 'eval_samples_per_second': 112.983, 'eval_steps_per_second': 7.067, 'epoch': 4.0}\n",
      "{'loss': 0.0307, 'learning_rate': 2.9163964798517833e-05, 'epoch': 4.17}\n",
      "{'loss': 0.026, 'learning_rate': 2.8006021306160262e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0287, 'learning_rate': 2.6848077813802684e-05, 'epoch': 4.63}\n",
      "{'loss': 0.028, 'learning_rate': 2.5690134321445113e-05, 'epoch': 4.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d427f92ad99c4d8389938d3d77aba3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.2857142857142857, 'recall': 0.12121212121212122, 'f1': 0.1702127659574468, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.30434782608695654, 'recall': 0.2413793103448276, 'f1': 0.2692307692307692, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8505359877488514, 'recall': 0.8880716341541414, 'f1': 0.8688986232790988, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9631083202511774, 'recall': 0.9570982839313572, 'f1': 0.9600938967136151, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5, 'recall': 0.4, 'f1': 0.4444444444444445, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6582132564841499, 'recall': 0.6842420611144397, 'f1': 0.6709753231492361, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7460097154753643, 'recall': 0.7818181818181819, 'f1': 0.7634943181818182, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8461070559610706, 'recall': 0.8629032258064516, 'f1': 0.8544226044226044, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory ./results\\checkpoint-10795 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1561158448457718, 'eval_art': {'precision': 0.2857142857142857, 'recall': 0.12121212121212122, 'f1': 0.1702127659574468, 'number': 33}, 'eval_eve': {'precision': 0.30434782608695654, 'recall': 0.2413793103448276, 'f1': 0.2692307692307692, 'number': 29}, 'eval_geo': {'precision': 0.8505359877488514, 'recall': 0.8880716341541414, 'f1': 0.8688986232790988, 'number': 3127}, 'eval_gpe': {'precision': 0.9631083202511774, 'recall': 0.9570982839313572, 'f1': 0.9600938967136151, 'number': 1282}, 'eval_nat': {'precision': 0.5, 'recall': 0.4, 'f1': 0.4444444444444445, 'number': 15}, 'eval_org': {'precision': 0.6582132564841499, 'recall': 0.6842420611144397, 'f1': 0.6709753231492361, 'number': 1669}, 'eval_per': {'precision': 0.7460097154753643, 'recall': 0.7818181818181819, 'f1': 0.7634943181818182, 'number': 1375}, 'eval_tim': {'precision': 0.8461070559610706, 'recall': 0.8629032258064516, 'f1': 0.8544226044226044, 'number': 1612}, 'eval_overall_precision': 0.810905612244898, 'eval_overall_recall': 0.8345001093852549, 'eval_overall_f1': 0.8225336927223721, 'eval_overall_accuracy': 0.9656895209546051, 'eval_runtime': 18.2115, 'eval_samples_per_second': 210.691, 'eval_steps_per_second': 13.178, 'epoch': 5.0}\n",
      "{'loss': 0.0231, 'learning_rate': 2.4532190829087543e-05, 'epoch': 5.09}\n",
      "{'loss': 0.0174, 'learning_rate': 2.3374247336729968e-05, 'epoch': 5.33}\n",
      "{'loss': 0.0172, 'learning_rate': 2.2216303844372397e-05, 'epoch': 5.56}\n",
      "{'loss': 0.019, 'learning_rate': 2.106067623899954e-05, 'epoch': 5.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155766a0f8a74c22a6f53a2a22295dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.2857142857142857, 'recall': 0.12121212121212122, 'f1': 0.1702127659574468, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.3684210526315789, 'recall': 0.2413793103448276, 'f1': 0.2916666666666667, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8408685306365259, 'recall': 0.9040614007035497, 'f1': 0.871320696563415, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9479724560061209, 'recall': 0.9664586583463338, 'f1': 0.9571263035921206, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5454545454545454, 'recall': 0.4, 'f1': 0.4615384615384615, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6949585194639438, 'recall': 0.652486518873577, 'f1': 0.6730531520395551, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7524271844660194, 'recall': 0.7890909090909091, 'f1': 0.7703230386936458, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8699690402476781, 'recall': 0.8715880893300249, 'f1': 0.8707778122094825, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory ./results\\checkpoint-12954 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1776302456855774, 'eval_art': {'precision': 0.2857142857142857, 'recall': 0.12121212121212122, 'f1': 0.1702127659574468, 'number': 33}, 'eval_eve': {'precision': 0.3684210526315789, 'recall': 0.2413793103448276, 'f1': 0.2916666666666667, 'number': 29}, 'eval_geo': {'precision': 0.8408685306365259, 'recall': 0.9040614007035497, 'f1': 0.871320696563415, 'number': 3127}, 'eval_gpe': {'precision': 0.9479724560061209, 'recall': 0.9664586583463338, 'f1': 0.9571263035921206, 'number': 1282}, 'eval_nat': {'precision': 0.5454545454545454, 'recall': 0.4, 'f1': 0.4615384615384615, 'number': 15}, 'eval_org': {'precision': 0.6949585194639438, 'recall': 0.652486518873577, 'f1': 0.6730531520395551, 'number': 1669}, 'eval_per': {'precision': 0.7524271844660194, 'recall': 0.7890909090909091, 'f1': 0.7703230386936458, 'number': 1375}, 'eval_tim': {'precision': 0.8699690402476781, 'recall': 0.8715880893300249, 'f1': 0.8707778122094825, 'number': 1612}, 'eval_overall_precision': 0.8206061904251901, 'eval_overall_recall': 0.8381098227958871, 'eval_overall_f1': 0.8292656529032956, 'eval_overall_accuracy': 0.966758455621783, 'eval_runtime': 15.8926, 'eval_samples_per_second': 241.434, 'eval_steps_per_second': 15.101, 'epoch': 6.0}\n",
      "{'loss': 0.0176, 'learning_rate': 1.990504863362668e-05, 'epoch': 6.02}\n",
      "{'loss': 0.0111, 'learning_rate': 1.8747105141269106e-05, 'epoch': 6.25}\n",
      "{'loss': 0.0118, 'learning_rate': 1.7589161648911535e-05, 'epoch': 6.48}\n",
      "{'loss': 0.0115, 'learning_rate': 1.643121815655396e-05, 'epoch': 6.72}\n",
      "{'loss': 0.0116, 'learning_rate': 1.5273274664196387e-05, 'epoch': 6.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9894395fa0de493fb41c13dfd84d26c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.4, 'recall': 0.12121212121212122, 'f1': 0.186046511627907, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.30434782608695654, 'recall': 0.2413793103448276, 'f1': 0.2692307692307692, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8405494177366378, 'recall': 0.9002238567316917, 'f1': 0.8693638048177889, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9660883280757098, 'recall': 0.9555382215288611, 'f1': 0.9607843137254901, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5, 'recall': 0.4, 'f1': 0.4444444444444445, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6785058175137784, 'recall': 0.6638705811863391, 'f1': 0.6711084191399151, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7519217330538085, 'recall': 0.7825454545454545, 'f1': 0.7669280114041339, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8651477832512315, 'recall': 0.8715880893300249, 'f1': 0.8683559950556241, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory ./results\\checkpoint-15113 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1858556568622589, 'eval_art': {'precision': 0.4, 'recall': 0.12121212121212122, 'f1': 0.186046511627907, 'number': 33}, 'eval_eve': {'precision': 0.30434782608695654, 'recall': 0.2413793103448276, 'f1': 0.2692307692307692, 'number': 29}, 'eval_geo': {'precision': 0.8405494177366378, 'recall': 0.9002238567316917, 'f1': 0.8693638048177889, 'number': 3127}, 'eval_gpe': {'precision': 0.9660883280757098, 'recall': 0.9555382215288611, 'f1': 0.9607843137254901, 'number': 1282}, 'eval_nat': {'precision': 0.5, 'recall': 0.4, 'f1': 0.4444444444444445, 'number': 15}, 'eval_org': {'precision': 0.6785058175137784, 'recall': 0.6638705811863391, 'f1': 0.6711084191399151, 'number': 1669}, 'eval_per': {'precision': 0.7519217330538085, 'recall': 0.7825454545454545, 'f1': 0.7669280114041339, 'number': 1375}, 'eval_tim': {'precision': 0.8651477832512315, 'recall': 0.8715880893300249, 'f1': 0.8683559950556241, 'number': 1612}, 'eval_overall_precision': 0.8177540106951872, 'eval_overall_recall': 0.8363596587180048, 'eval_overall_f1': 0.826952195544019, 'eval_overall_accuracy': 0.9665376973753006, 'eval_runtime': 16.7894, 'eval_samples_per_second': 228.537, 'eval_steps_per_second': 14.295, 'epoch': 7.0}\n",
      "{'loss': 0.0075, 'learning_rate': 1.411764705882353e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0075, 'learning_rate': 1.2959703566465959e-05, 'epoch': 7.41}\n",
      "{'loss': 0.0072, 'learning_rate': 1.1801760074108385e-05, 'epoch': 7.64}\n",
      "{'loss': 0.0069, 'learning_rate': 1.064381658175081e-05, 'epoch': 7.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfb7605c53a43a89cb42fa722bc272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.3076923076923077, 'recall': 0.12121212121212122, 'f1': 0.17391304347826086, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.35, 'recall': 0.2413793103448276, 'f1': 0.2857142857142857, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8532110091743119, 'recall': 0.8922289734569875, 'f1': 0.8722838830701891, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9461123941493457, 'recall': 0.9586583463338534, 'f1': 0.9523440526927547, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5333333333333333, 'recall': 0.5333333333333333, 'f1': 0.5333333333333333, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6619469026548672, 'recall': 0.6722588376273217, 'f1': 0.6670630202140309, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7565463552724699, 'recall': 0.7774545454545455, 'f1': 0.7668579626972741, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8694841516469857, 'recall': 0.8678660049627791, 'f1': 0.8686743247438682, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20449940860271454, 'eval_art': {'precision': 0.3076923076923077, 'recall': 0.12121212121212122, 'f1': 0.17391304347826086, 'number': 33}, 'eval_eve': {'precision': 0.35, 'recall': 0.2413793103448276, 'f1': 0.2857142857142857, 'number': 29}, 'eval_geo': {'precision': 0.8532110091743119, 'recall': 0.8922289734569875, 'f1': 0.8722838830701891, 'number': 3127}, 'eval_gpe': {'precision': 0.9461123941493457, 'recall': 0.9586583463338534, 'f1': 0.9523440526927547, 'number': 1282}, 'eval_nat': {'precision': 0.5333333333333333, 'recall': 0.5333333333333333, 'f1': 0.5333333333333333, 'number': 15}, 'eval_org': {'precision': 0.6619469026548672, 'recall': 0.6722588376273217, 'f1': 0.6670630202140309, 'number': 1669}, 'eval_per': {'precision': 0.7565463552724699, 'recall': 0.7774545454545455, 'f1': 0.7668579626972741, 'number': 1375}, 'eval_tim': {'precision': 0.8694841516469857, 'recall': 0.8678660049627791, 'f1': 0.8686743247438682, 'number': 1612}, 'eval_overall_precision': 0.8172273409042211, 'eval_overall_recall': 0.8343907241303872, 'eval_overall_f1': 0.8257198527819873, 'eval_overall_accuracy': 0.9663866522592863, 'eval_runtime': 123.9113, 'eval_samples_per_second': 30.966, 'eval_steps_per_second': 1.937, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results\\checkpoint-17272 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0059, 'learning_rate': 9.485873089393238e-06, 'epoch': 8.11}\n",
      "{'loss': 0.005, 'learning_rate': 8.327929597035665e-06, 'epoch': 8.34}\n",
      "{'loss': 0.0049, 'learning_rate': 7.169986104678092e-06, 'epoch': 8.57}\n",
      "{'loss': 0.0046, 'learning_rate': 6.012042612320519e-06, 'epoch': 8.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062af44db37a46bdade07ff16a9cd578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.2857142857142857, 'recall': 0.12121212121212122, 'f1': 0.1702127659574468, 'number': 33}\" of type <class 'dict'> for key \"eval/art\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.2727272727272727, 'recall': 0.20689655172413793, 'f1': 0.23529411764705882, 'number': 29}\" of type <class 'dict'> for key \"eval/eve\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8543127095397745, 'recall': 0.8963863127598337, 'f1': 0.8748439450686641, 'number': 3127}\" of type <class 'dict'> for key \"eval/geo\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.947652040030793, 'recall': 0.9602184087363494, 'f1': 0.9538938395970554, 'number': 1282}\" of type <class 'dict'> for key \"eval/gpe\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5, 'recall': 0.4666666666666667, 'f1': 0.4827586206896552, 'number': 15}\" of type <class 'dict'> for key \"eval/nat\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6743480897513645, 'recall': 0.6662672258837627, 'f1': 0.6702833031946956, 'number': 1669}\" of type <class 'dict'> for key \"eval/org\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7489597780859917, 'recall': 0.7854545454545454, 'f1': 0.7667731629392971, 'number': 1375}\" of type <class 'dict'> for key \"eval/per\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8627935723114957, 'recall': 0.8660049627791563, 'f1': 0.8643962848297212, 'number': 1612}\" of type <class 'dict'> for key \"eval/tim\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Checkpoint destination directory ./results\\checkpoint-19431 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21622690558433533, 'eval_art': {'precision': 0.2857142857142857, 'recall': 0.12121212121212122, 'f1': 0.1702127659574468, 'number': 33}, 'eval_eve': {'precision': 0.2727272727272727, 'recall': 0.20689655172413793, 'f1': 0.23529411764705882, 'number': 29}, 'eval_geo': {'precision': 0.8543127095397745, 'recall': 0.8963863127598337, 'f1': 0.8748439450686641, 'number': 3127}, 'eval_gpe': {'precision': 0.947652040030793, 'recall': 0.9602184087363494, 'f1': 0.9538938395970554, 'number': 1282}, 'eval_nat': {'precision': 0.5, 'recall': 0.4666666666666667, 'f1': 0.4827586206896552, 'number': 15}, 'eval_org': {'precision': 0.6743480897513645, 'recall': 0.6662672258837627, 'f1': 0.6702833031946956, 'number': 1669}, 'eval_per': {'precision': 0.7489597780859917, 'recall': 0.7854545454545454, 'f1': 0.7667731629392971, 'number': 1375}, 'eval_tim': {'precision': 0.8627935723114957, 'recall': 0.8660049627791563, 'f1': 0.8643962848297212, 'number': 1612}, 'eval_overall_precision': 0.8179676624906307, 'eval_overall_recall': 0.8355939619339313, 'eval_overall_f1': 0.8266868675937449, 'eval_overall_accuracy': 0.9664796030999105, 'eval_runtime': 80.7653, 'eval_samples_per_second': 47.508, 'eval_steps_per_second': 2.972, 'epoch': 9.0}\n",
      "{'loss': 0.0043, 'learning_rate': 4.854099119962946e-06, 'epoch': 9.03}\n",
      "{'loss': 0.0031, 'learning_rate': 3.6984715145900885e-06, 'epoch': 9.26}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2tag[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[id2tag[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "                        for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    # logging_steps=50,\n",
    "    fp16=True  # mixed precision for faster training on GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00eaefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a36b1",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77f4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification\n",
    "\n",
    "# Load saved model\n",
    "model_path = \"results/checkpoint-19431\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c22b6666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344ee2e5faae4d9cbca7b322328629f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21233141422271729, 'eval_precision': 0.8171621862204024, 'eval_recall': 0.8329930583911801, 'eval_f1': 0.8250016850524613, 'eval_accuracy': 0.9673390519695527, 'eval_runtime': 38.4888, 'eval_samples_per_second': 249.215, 'eval_steps_per_second': 31.152}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "label_list = model.config.id2label\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c02b4b",
   "metadata": {},
   "source": [
    "### Results - after 9 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb86f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     'eval_loss': 0.21233141422271729, \n",
    "#     'eval_precision': 0.8171621862204024, \n",
    "#     'eval_recall': 0.8329930583911801, \n",
    "#     'eval_f1': 0.8250016850524613, \n",
    "#     'eval_accuracy': 0.9673390519695527, \n",
    "#     'eval_runtime': 38.4888, \n",
    "#     'eval_samples_per_second': 249.215, \n",
    "#     'eval_steps_per_second': 31.152\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6b2d8",
   "metadata": {},
   "source": [
    "### Prediction script for api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8ef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('President', 'O'), ('Karzai', 'B-per'), ('thanked', 'I-per'), ('his', 'I-per'), ('allies', 'I-per'), ('for', 'O'), ('their', 'O'), ('help', 'O'), ('in', 'O'), ('battling', 'O'), ('terrorism', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "sentence = [\"Michael\", \"Jackson\", \"lives\", \"in\", \"New\", \"Delhi\"]\n",
    "sentence = ['They','say','not','all','of','the','rockets','exploded','upon','impact','.']\n",
    "sentence = ['The', 'former', 'prime', 'minister', 'was', 'replaced', 'October', '19', 'by', 'hardliner', 'Lieutenant', 'General', 'Soe', 'Win', '.']\n",
    "sentence = ['President', 'Karzai', 'thanked', 'his', 'allies', 'for', 'their', 'help', 'in', 'battling', 'terrorism', '.']\n",
    "\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    is_split_into_words=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ").to(device)   # <<< move tensors to GPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "\n",
    "predictions = outputs.argmax(-1).squeeze().tolist()\n",
    "predicted_labels = [model.config.id2label[p] for p in predictions[:len(sentence)]]\n",
    "\n",
    "print(list(zip(sentence, predicted_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7f7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ner_venv_py312)",
   "language": "python",
   "name": "ner_venv_py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
