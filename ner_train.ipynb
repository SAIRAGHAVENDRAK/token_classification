{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410ae221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a1670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1  Sentence: 1             of   IN   O\n",
      "2  Sentence: 1  demonstrators  NNS   O\n",
      "3  Sentence: 1           have  VBP   O\n",
      "4  Sentence: 1        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ner_dataset.csv\", encoding=\"latin1\").ffill()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61624fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 47959\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# STEP 2: Group Words by Sentence\n",
    "# --------------------------\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "current_labels = []\n",
    "prev_sent_id = df.iloc[0][\"Sentence #\"]\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sent_id = row[\"Sentence #\"]\n",
    "    word = row[\"Word\"]\n",
    "    tag = row[\"Tag\"]\n",
    "\n",
    "    if sent_id != prev_sent_id:\n",
    "        sentences.append((current_sentence, current_labels))\n",
    "        current_sentence = []\n",
    "        current_labels = []\n",
    "        prev_sent_id = sent_id\n",
    "\n",
    "    current_sentence.append(word)\n",
    "    current_labels.append(tag)\n",
    "\n",
    "# add last sentence\n",
    "if current_sentence:\n",
    "    sentences.append((current_sentence, current_labels))\n",
    "\n",
    "print(f\"Total sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1ccd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# STEP 3: Convert to spaCy Example Format\n",
    "# --------------------------\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def create_docs(sentences):\n",
    "    docs = []\n",
    "    for tokens, labels in sentences:\n",
    "        doc = nlp.make_doc(\" \".join(tokens))\n",
    "        ents = []\n",
    "        start = 0\n",
    "        for token, label in zip(tokens, labels):\n",
    "            end = start + len(token)\n",
    "            if label != \"O\":\n",
    "                label_type = label.split(\"-\")[-1]  # B-PER → PER\n",
    "                span = doc.char_span(start, end, label=label_type, alignment_mode=\"contract\")\n",
    "                if span:\n",
    "                    ents.append(span)\n",
    "            start = end + 1  # +1 for space\n",
    "        doc.ents = ents\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e9e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 38367, Val: 4796, Test: 4796\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# STEP 4: Train/Val/Test Split\n",
    "# --------------------------\n",
    "train_sents, test_sents = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "val_sents, test_sents = train_test_split(test_sents, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_sents)}, Val: {len(val_sents)}, Test: {len(test_sents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "843a24ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved train.spacy, val.spacy, test.spacy\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# STEP 5: Save to DocBin\n",
    "# --------------------------\n",
    "def save_to_docbin(sentences, path):\n",
    "    docs = create_docs(sentences)\n",
    "    db = DocBin(docs=docs)\n",
    "    db.to_disk(path)\n",
    "\n",
    "save_to_docbin(train_sents, \"data/train.spacy\")\n",
    "save_to_docbin(val_sents, \"data/val.spacy\")\n",
    "save_to_docbin(test_sents, \"data/test.spacy\")\n",
    "\n",
    "print(\"✅ Saved train.spacy, val.spacy, test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b712fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        O        \n",
      "58         O        \n",
      "-          O        \n",
      "year       O        \n",
      "-          O        \n",
      "old        O        \n",
      "former     O        \n",
      "analyst    O        \n",
      "says       O        \n",
      "he         O        \n",
      "provided   O        \n",
      "information O        \n",
      "to         O        \n",
      "an         O        \n",
      "official   O        \n",
      "at         O        \n",
      "the        O        \n",
      "Israeli    B   gpe  \n",
      "embassy    O        \n",
      "and        O        \n",
      "to         O        \n",
      "two        O        \n",
      "members    O        \n",
      "of         O        \n",
      "a          O        \n",
      "lobbying   O        \n",
      "group      O        \n",
      "called     O        \n",
      "the        O        \n",
      "American   O        \n",
      "Israel     B   geo  \n",
      "Public     B   org  \n",
      "Affairs    B   org  \n",
      "Committee  B   org  \n",
      ".          O        \n",
      "Entities: [('Israeli', 'gpe'), ('Israel', 'geo'), ('Public', 'org'), ('Affairs', 'org'), ('Committee', 'org')]\n"
     ]
    }
   ],
   "source": [
    "# pretty-print sample train data\n",
    "print(create_docs([train_sents[0]])[0].to_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3584e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 38367 train docs, 4796 val docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ragha\\Documents\\Work\\Projects\\ner_venv_py312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1, Losses: {'transformer': 0.0, 'ner': 51975.42819693507}\n",
      "✅ Model saved to ./ner_model\n",
      "Validation Precision: 0.85 (13376/15722)\n",
      "Entities: [('Elon', 'per'), ('Musk', 'org'), ('USA', 'org')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# --------------------------\n",
    "# STEP 1: Check GPU\n",
    "# --------------------------\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --------------------------\n",
    "# STEP 2: Load Dataset (.spacy files from previous step)\n",
    "# --------------------------\n",
    "def load_docbin(path, nlp):\n",
    "    db = DocBin().from_disk(path)\n",
    "    return list(db.get_docs(nlp.vocab))\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "train_docs = load_docbin(\"data/train.spacy\", nlp)\n",
    "val_docs = load_docbin(\"data/val.spacy\", nlp)\n",
    "\n",
    "print(f\"Loaded: {len(train_docs)} train docs, {len(val_docs)} val docs\")\n",
    "\n",
    "# --------------------------\n",
    "# STEP 3: Build Pipeline with Transformer + NER\n",
    "# --------------------------\n",
    "# Add transformer (DistilBERT fits well in 8GB VRAM)\n",
    "nlp.add_pipe(\"transformer\", config={\"model\": {\"name\": \"distilbert-base-uncased\"}})\n",
    "ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "# Add labels from training data\n",
    "for doc in train_docs:\n",
    "    for ent in doc.ents:\n",
    "        ner.add_label(ent.label_)\n",
    "\n",
    "# --------------------------\n",
    "# STEP 4: Convert Docs to Examples\n",
    "# --------------------------\n",
    "train_examples = [Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]}) for doc in train_docs]\n",
    "val_examples = [Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]}) for doc in val_docs]\n",
    "\n",
    "# --------------------------\n",
    "# STEP 5: Initialize & Train\n",
    "# --------------------------\n",
    "optimizer = nlp.initialize(lambda: train_examples)\n",
    "\n",
    "n_iter = 1   # increase for full training (start small for testing)\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(train_examples)\n",
    "    losses = {}\n",
    "    for batch in spacy.util.minibatch(train_examples, size=8):  # batch size fits 8GB VRAM\n",
    "        nlp.update(batch, sgd=optimizer, losses=losses, drop=0.1)\n",
    "    print(f\"Iteration {i+1}/{n_iter}, Losses: {losses}\")\n",
    "\n",
    "# --------------------------\n",
    "# STEP 6: Save Model\n",
    "# --------------------------\n",
    "output_dir = \"./ner_model\"\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"✅ Model saved to {output_dir}\")\n",
    "\n",
    "# --------------------------\n",
    "# STEP 7: Evaluate on Validation Set\n",
    "# --------------------------\n",
    "nlp2 = spacy.load(output_dir)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for ex in val_examples:\n",
    "    doc = nlp2(ex.text)\n",
    "    pred = set([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    gold = set([(ent.text, ent.label_) for ent in ex.reference.ents])\n",
    "    total += len(gold)\n",
    "    correct += len(pred & gold)\n",
    "\n",
    "print(f\"Validation Precision: {correct/total:.2f} ({correct}/{total})\")\n",
    "\n",
    "# --------------------------\n",
    "# STEP 8: Quick Test\n",
    "# --------------------------\n",
    "doc = nlp2(\"Elon Musk founded SpaceX in the USA.\")\n",
    "print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6ee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ner_venv_py312)",
   "language": "python",
   "name": "ner_venv_py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
